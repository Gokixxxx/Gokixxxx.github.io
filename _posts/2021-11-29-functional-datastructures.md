---
layout: post
title: 函数式数据结构
subtitle: ¿
tags: 数据结构
show: true
---

noip结束之后开始翻论文了。翻了一个Higman引理，感觉非常不成功(已经看不动了，等我英语过了114514级再看)，然后想起来可持久化并查集，就随机了一篇来看。

还是很没用啊!

https://doc.lagout.org/programmation/Functional%20Programming/Chris_Okasaki-Purely_Functional_Data_Structures-Cambridge_University_Press%281998%29.pdf

完全不可持久化的数据结构称为短暂(ephemeral)的。这个称呼还挺好?

最有趣的事情是，它使均摊数据结构的可持久化变为了可能。

先假设你会Haskell。如果你不会，那么没有关系，因为Haskell很可读。如果实在读不懂，请参阅http://learnyouahaskell.com/chapters。

原文还讲了一个可持久化双端队列......但是我懒得看了（

实际上后面还有很多东西没看......但是我要开卷了。

## 记忆化

要卡一般的可持久化均摊数据结构，方法就是按着一个代价很大的操作不停进行。

记忆化 是说函数式数据结构只做计算和复制而不会修改，于是我们可以让每个计算只进行一次。这样允许记忆化的部分，它的代价就可以省下来，可持久化带来的复杂度增加就仅限于不能记忆化的部分。

你发现为了实现记忆化，没有什么不用映射数据结构的好办法。如果允许动态完美哈希，或者认为有无限的空间可以用......我们是不是应该整点practical的东西?

那就只能胡乱处理这个东西了。拿到问题再说怎么办吧（

另外，最经典的记忆化是并查集的路径压缩。

## 懒惰求值，$$O(1)$$插入的可持久化二项堆

把计算后延直到真的需要它的结果的时候。这个是很好实现的。

考虑二项堆(没有decrease-key)，它本来是均摊$$O(1)$$严格$$O(\log n)$$插入，严格$$O(\log n)$$删除最小值，但是如果支持可持久化，它的插入就会取到最坏，方法也很简单，不停地在有$$2^k-1$$个元素时插入即可。那么怎么办呢?

如果使用懒惰求值，我们就可以把插入延后到后面的删除最小值操作，因为直到那时才需要整个堆的信息。而推掉这个插入的复杂度是$$O(\log n)$$，删除最小值的复杂度是$$O(\log n)$$，此时插入的复杂度就被删除最小值的复杂度吃掉了。再加上记忆化，可持久化二项堆就会变成$$O(1)$$插入。注意这个插入是严格的，但是删除最小值是均摊的!这确实是可持久化的均摊数据结构。

这听起来还是很对的，但是我们还有几个问题。接下来依次讨论它们。

-----

一个操作的代价包括三部分 : 必然要付出的部分$$c_1$$，可以记忆化的部分$$c_2$$(之前记住了就不必再付出)，势函数的变化量$$\Delta\Phi$$。第一部分称为局部(unshared)代价，第二部分是全局(shared)代价。

举个例子，splay查询一次，需要付出深度的代价找到那个点(这是记忆化了也不能优化的)，然后付出深度的代价把那个点转上来(这是可以记忆化的)，然后势函数会变化。

一个操作的局部代价可能会付出多次，但是相同的全局代价只需要付出一次。于是总的实际代价是把$$c_1$$全都按照最坏情况算，然后把$$c_2$$都加起来。注意到$$c_2$$并不会被卡到最坏，于是此时势函数的变化还是可以摊掉$$c_2$$，也就是说一个操作的均摊复杂度将会是$$\max(c_1,c_2+\Delta\Phi)$$。

好像不完全对，因为一个势能仍然可能在后面的多个状态中被使用，毕竟消耗势能的方法不是唯一的。

考虑强行修正这个想法，我们钦点每一单位的势能表示一单位时间的懒惰求值，势函数表示的是懒惰求值所需总时间的一个上限(不必是紧的)。当我们创建懒惰求值的时候，会提升这个势能，而进行懒惰求值的时候则会释放势能。此时我们重新定义一个操作的均摊代价是$$\max(c_1,c_2-\Delta\Phi)$$，这就对了。

注意到原来势能分析的势函数估计的是前面省下的势能下限，而现在的势函数估计还没有付出的代价上限。这两个是完全对称的，于是它们几乎是等价的。

不可持久化的均摊数据结构比如splay，主要是因为它的实际代价可能很大，而实际代价是可能需要多次付出的，势函数的增减无法摊掉这一部分。比如我找到一个操作序列，可以得到一条长链下面挂着很多点，那么反复操作下面挂着的点，它就没了。

-----

现在还是考虑可持久化二项堆，它的势函数是元素个数二进制表示中$$1$$的个数。

如果我们进行一次插入的时候，末尾有$$k$$个$$1$$，那么我们会把它们全变成$$0$$，并在$$k+1$$处得到一个新的$$1$$，势函数增加了$$k-1$$，而进行的求值代价是$$O(k)$$。这就分析完了。

-----

现在问题是怎么实现插入的懒惰求值和记忆化。我们给每个版本开一个可持久化单链表，存储所有影响这个版本并且还没推掉的插入操作。如果现在进行了一个extract-min，那么所有插入都要被推掉，也就是说这个版本会把它基于的版本的链表拿过来推一遍。

为了避免尝试去推已经推过的操作，我们从最新的插入开始往前找，直到有一个插入推过了，那么我们把推掉这个插入的版本拿过来，然后插入新的这些，一边插一边进行记忆化。总之就是用插入来索引插入之后得到的版本。

## 可持久化配对堆

直接上配对堆会死，因为如果我先插了很多，接下来我对于每个版本删一次最小值，那么复杂度就飞了。

别左儿子右兄弟了。我们每个点维护把所有儿子配对之后得到的儿子，而为了实现这个配对我们再维护这个点和它的哪个兄弟配对。

$$\mathrm{link}(u,v)$$($$u$$的值比$$v$$小)的时候，如果$$u$$没有左儿子，那么直接把$$v$$挂在$$u$$下面，否则递归地把$$u$$的左儿子和$$v$$合并，再把它俩合并到和$$u$$配对的兄弟，当然所有的合并都是懒惰求值的。这看起来可能比较奇怪，我们是不是应该把这个写出来看看?

```haskell
module LazyPairingHeap (PairingHeap) where
    data PairingHeap a = E | T a (PairingHeap a) (PairingHeap a)

    link (T x E m) a = T x a m
    link (T x b m) a = T x E (merge (merge a b) m)

    merge a E = a
    merge E b = b
    merge a@(T x _ _) b@(T y _ _) = if x<=y then link a b else link b a

    --...(insert,findMin,extractMin)
```

实际上它在做的事情就是把一个代价很大的合并拆成了若干个代价$$O(1)$$的合并。

要实现这个记忆化，似乎也不困难。我们只需要保留merge的懒惰求值。当我们需要推一个懒惰求值的时候，必然是访问了一个点的儿子或者配对的兄弟，

这个东西复杂度怎么看都非常见鬼，分析起来显然也会很困难(它和不可持久化的版本有区别)，但是书中猜测它是对的，并且做了一些实测。后来也许被证明了，因为我看到racket的文档里写的很肯定（

## Scheduling(试译 求值规划)，严格$$O(1)$$插入的可持久化二项堆

通过让懒惰求值不要太懒，把均摊变成严格。

不过这东西实践意义不大，毕竟均摊的可持久化数据结构已经足够强力了。

求值规划实在是非常简单。对于可持久化二项堆，每次操作之后势能减少$$k-1$$，但是我们创建了$$k+1$$个懒惰求值，于是我们开一个队列，处理掉前面产生的两个懒惰求值即可。这个 两个 是根据每单位势能的计算量决定的，需要根据具体问题来看（